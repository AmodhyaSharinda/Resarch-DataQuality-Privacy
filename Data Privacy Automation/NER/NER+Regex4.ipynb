{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f705773c",
   "metadata": {},
   "source": [
    "NER - PERSON, ORG, DATE, LOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a7a14df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\Desktop\\Research\\Data Privacy Automation\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "W0105 08:57:12.953000 13364 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "model_name = \"Davlan/xlm-roberta-base-ner-hrl\"\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "ner = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\",\n",
    "    ignore_labels=[]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8778d09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ner(text: str, min_confidence=0.6):\n",
    "    raw = ner(text)\n",
    "    results = []\n",
    "\n",
    "    for e in raw:\n",
    "        if e[\"score\"] >= min_confidence and e[\"entity_group\"] != \"O\":\n",
    "            # Compute start/end manually\n",
    "            value = e[\"word\"]\n",
    "            start = text.find(value)\n",
    "            end = start + len(value)\n",
    "\n",
    "            results.append({\n",
    "                \"entity\": e[\"entity_group\"],  # PER, LOC, ORG, etc.\n",
    "                \"value\": value,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"source\": \"ner\",\n",
    "                \"confidence\": round(float(e[\"score\"]), 3)\n",
    "            })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dd0f1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'O', 1: 'B-DATE', 2: 'I-DATE', 3: 'B-PER', 4: 'I-PER', 5: 'B-ORG', 6: 'I-ORG', 7: 'B-LOC', 8: 'I-LOC'}\n"
     ]
    }
   ],
   "source": [
    "# Print all entity labels\n",
    "labels = model.config.id2label\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d9a08",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687fdde5",
   "metadata": {},
   "source": [
    "NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17494b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model & tokenizer loaded correctly\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "MODEL_PATH = r\"./xlmr_ner_model\"\n",
    "\n",
    "# ---- Load tokenizer.json DIRECTLY ----\n",
    "raw_tokenizer = Tokenizer.from_file(\n",
    "    f\"{MODEL_PATH}/tokenizer.json\"\n",
    ")\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=raw_tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    sep_token=\"</s>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<s>\",\n",
    "    mask_token=\"<mask>\",\n",
    ")\n",
    "\n",
    "# ---- Load model ----\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"✅ Model & tokenizer loaded correctly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7524077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ner(text):\n",
    "    words = text.split()\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        words,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    word_ids = encoding.word_ids()\n",
    "    encoding = {k: v.to(device) for k, v in encoding.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "\n",
    "    preds = outputs.logits.argmax(dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    results = []\n",
    "    seen = set()\n",
    "\n",
    "    for pred, wid in zip(preds, word_ids):\n",
    "        if wid is None or wid in seen:\n",
    "            continue\n",
    "        seen.add(wid)\n",
    "        results.append((words[wid], model.config.id2label[pred]))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4f60c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_MAP = {\n",
    "    \"PERSON\": \"PER\",\n",
    "    \"EMAIL\": \"EMAIL\",\n",
    "    \"MARITAL_STATUS\": \"MARITAL_STATUS\",\n",
    "    \"PASSWORD\": \"PASSWORD\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8113b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bio_to_spans(tokens_and_labels, text, source=\"ner2\"):\n",
    "    entities = []\n",
    "    current = None\n",
    "    text_ptr = 0\n",
    "\n",
    "    for token, label in tokens_and_labels:\n",
    "        start = text.find(token, text_ptr)\n",
    "        if start == -1:\n",
    "            continue\n",
    "        end = start + len(token)\n",
    "        text_ptr = end\n",
    "\n",
    "        if label.startswith(\"B-\"):\n",
    "            if current:\n",
    "                entities.append(current)\n",
    "\n",
    "            CONFIDENCE_FALLBACK = 0.85\n",
    "\n",
    "            current = {\n",
    "                \"entity\": label[2:],\n",
    "                \"value\": token,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"source\": source,\n",
    "                \"confidence\": CONFIDENCE_FALLBACK\n",
    "            }\n",
    "\n",
    "\n",
    "        elif label.startswith(\"I-\") and current:\n",
    "            current[\"value\"] += \" \" + token\n",
    "            current[\"end\"] = end\n",
    "\n",
    "        else:\n",
    "            if current:\n",
    "                entities.append(current)\n",
    "                current = None\n",
    "\n",
    "    if current:\n",
    "        entities.append(current)\n",
    "\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aa3ae2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'PERSON',\n",
       "  'value': 'Dinithi Rajapaksha',\n",
       "  'start': 13,\n",
       "  'end': 31,\n",
       "  'source': 'ner2',\n",
       "  'confidence': 0.85},\n",
       " {'entity': 'PASSWORD',\n",
       "  'value': 'uf5676ADS',\n",
       "  'start': 36,\n",
       "  'end': 45,\n",
       "  'source': 'ner2',\n",
       "  'confidence': 0.85},\n",
       " {'entity': 'EMAIL',\n",
       "  'value': 'dinithi@gmail.com',\n",
       "  'start': 56,\n",
       "  'end': 73,\n",
       "  'source': 'ner2',\n",
       "  'confidence': 0.85}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Patient name Dinithi Rajapaksha has uf5676ADS and email dinithi@gmail.com\"\n",
    "\n",
    "def run_ner2(text):\n",
    "    tokens_labels = predict_ner(text)\n",
    "    entities = bio_to_spans(tokens_labels, text)\n",
    "    return entities\n",
    "\n",
    "run_ner2(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457096c0",
   "metadata": {},
   "source": [
    "NIC DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "618a9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "# =========================\n",
    "# Regex patterns\n",
    "# =========================\n",
    "OLD_NIC_REGEX = re.compile(r\"\\b\\d{9}[VXvx]\\b\")\n",
    "NEW_NIC_REGEX = re.compile(r\"\\b\\d{12}\\b\")\n",
    "CURRENT_YEAR = datetime.now().year\n",
    "\n",
    "# =========================\n",
    "# Validation helpers\n",
    "# =========================\n",
    "def is_valid_day_of_year(day: int) -> bool:\n",
    "    return (1 <= day <= 366) or (501 <= day <= 866)\n",
    "\n",
    "def validate_old_nic(nic: str) -> bool:\n",
    "    try:\n",
    "        yy = int(nic[0:2])\n",
    "        day = int(nic[2:5])\n",
    "\n",
    "        # Day-of-year check (HARD)\n",
    "        if not is_valid_day_of_year(day):\n",
    "            return False\n",
    "\n",
    "        # Infer full year (SOFT)\n",
    "        inferred_year = 1900 + yy if yy > (CURRENT_YEAR % 100) else 2000 + yy\n",
    "\n",
    "        # Year sanity check\n",
    "        if inferred_year < 1900 or inferred_year > CURRENT_YEAR:\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def validate_new_nic(nic: str) -> bool:\n",
    "    try:\n",
    "        # Handle 12 or 13 digit variants\n",
    "        if len(nic) == 12:\n",
    "            year = int(nic[0:4])\n",
    "            day = int(nic[4:7])\n",
    "        elif len(nic) == 13:\n",
    "            year = int(nic[1:5])\n",
    "            day = int(nic[5:8])\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "        # HARD checks\n",
    "        if not is_valid_day_of_year(day):\n",
    "            return False\n",
    "\n",
    "        if not (1900 <= year <= CURRENT_YEAR):\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Main NIC extractor\n",
    "# =========================\n",
    "def extract_nic(text: str):\n",
    "    results = []\n",
    "\n",
    "    # OLD NICs\n",
    "    for match in OLD_NIC_REGEX.finditer(text):\n",
    "        nic = match.group()\n",
    "        if validate_old_nic(nic):\n",
    "            results.append({\n",
    "                \"entity\": \"NIC\",\n",
    "                \"value\": nic,\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end(),\n",
    "                \"source\": \"regex\",\n",
    "                \"confidence\": 1.0\n",
    "            })\n",
    "\n",
    "    # NEW NICs\n",
    "    for match in NEW_NIC_REGEX.finditer(text):\n",
    "        nic = match.group()\n",
    "        if validate_new_nic(nic):\n",
    "            results.append({\n",
    "                \"entity\": \"NIC\",\n",
    "                \"value\": nic,\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end(),\n",
    "                \"source\": \"regex\",\n",
    "                \"confidence\": 1.0\n",
    "            })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0b11a",
   "metadata": {},
   "source": [
    "CREDIT CARD NO DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6a4db20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "CREDIT_CARD_REGEX = re.compile(r\"\\b\\d{13,19}\\b\")\n",
    "\n",
    "def luhn_check(card_number: str) -> bool:\n",
    "    total = 0\n",
    "    reverse_digits = card_number[::-1]\n",
    "\n",
    "    for i, digit in enumerate(reverse_digits):\n",
    "        n = int(digit)\n",
    "\n",
    "        if i % 2 == 1:  # double every second digit\n",
    "            n *= 2\n",
    "            if n > 9:\n",
    "                n -= 9\n",
    "\n",
    "        total += n\n",
    "\n",
    "    return total % 10 == 0\n",
    "\n",
    "def valid_card_prefix(card_number: str) -> bool:\n",
    "    length = len(card_number)\n",
    "\n",
    "    # Visa\n",
    "    if card_number.startswith(\"4\") and length in (16, 19):\n",
    "        return True\n",
    "\n",
    "    # Mastercard\n",
    "    if length in (16, 19):\n",
    "        prefix2 = int(card_number[:2])\n",
    "        prefix6 = int(card_number[:6])\n",
    "\n",
    "        if 51 <= prefix2 <= 55:\n",
    "            return True\n",
    "        if 222100 <= prefix6 <= 272099:\n",
    "            return True\n",
    "\n",
    "    # American Express\n",
    "    if length == 15 and card_number[:2] in (\"34\", \"37\"):\n",
    "        return True\n",
    "\n",
    "    # Discover (simplified but correct)\n",
    "    if length in (14, 16):\n",
    "        if card_number.startswith((\"6011\", \"65\")):\n",
    "            return True\n",
    "        if 622126 <= int(card_number[:6]) <= 623796:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def extract_credit_card(text: str):\n",
    "    results = []\n",
    "\n",
    "    for match in CREDIT_CARD_REGEX.finditer(text):\n",
    "        token = match.group()\n",
    "\n",
    "        try:\n",
    "            if not valid_card_prefix(token):\n",
    "                continue\n",
    "\n",
    "            if not luhn_check(token):\n",
    "                continue\n",
    "\n",
    "            results.append({\n",
    "                \"entity\": \"CREDIT_CARD\",\n",
    "                \"value\": token,\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end(),\n",
    "                \"source\": \"regex\",\n",
    "                \"confidence\": 1.0\n",
    "            })\n",
    "\n",
    "\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3afa39",
   "metadata": {},
   "source": [
    "BANK ACCOUNT NO DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfa62971",
   "metadata": {},
   "outputs": [],
   "source": [
    "BANK_ACCOUNT_REGEX = re.compile(r\"\\b\\d{10,18}\\b\")\n",
    "\n",
    "VALID_BANK_ACCOUNT_LENGTHS = {\n",
    "    11, 12, 13, 14, 15, 16, 18\n",
    "}\n",
    "\n",
    "KNOWN_BANK_PREFIXES = (\n",
    "    \"01\", \"10\", \"18\", \"88\",   # Sampath / SCB\n",
    "    \"2042\",                   # People's Bank\n",
    "    \"1\", \"2\"                  # PABC\n",
    ")\n",
    "\n",
    "\n",
    "def extract_bank_account(text: str):\n",
    "    results = []\n",
    "\n",
    "    for match in BANK_ACCOUNT_REGEX.finditer(text):\n",
    "        token = match.group()\n",
    "\n",
    "        # Length validation (HARD)\n",
    "        if len(token) not in VALID_BANK_ACCOUNT_LENGTHS:\n",
    "            continue\n",
    "\n",
    "        # Optional prefix check (SOFT)\n",
    "        prefix_match = token.startswith(KNOWN_BANK_PREFIXES)\n",
    "\n",
    "        results.append({\n",
    "            \"entity\": \"BANK_ACCOUNT\",\n",
    "            \"value\": token,\n",
    "            \"start\": match.start(),\n",
    "            \"end\": match.end(),\n",
    "            \"source\": \"regex\",\n",
    "            \"confidence\": 0.6 if prefix_match else 0.4\n",
    "        })\n",
    "\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d51513",
   "metadata": {},
   "source": [
    "PHONE NUMBER DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ed31584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# --------------------------------------------------\n",
    "# VALID CODES\n",
    "# --------------------------------------------------\n",
    "\n",
    "VALID_LANDLINE_AREA_CODES = {\n",
    "    \"011\", \"036\", \"031\", \"033\", \"038\", \"034\",\n",
    "    \"054\", \"081\", \"051\", \"052\", \"066\",\n",
    "    \"091\", \"041\", \"047\",\n",
    "    \"032\", \"037\",\n",
    "    \"021\", \"023\", \"024\",\n",
    "    \"063\", \"067\", \"065\", \"026\",\n",
    "    \"025\", \"027\",\n",
    "    \"055\", \"057\",\n",
    "    \"045\", \"035\"\n",
    "}\n",
    "\n",
    "VALID_MOBILE_OPERATOR_CODES = {\n",
    "    \"070\", \"071\", \"072\", \"074\",\n",
    "    \"075\", \"076\", \"077\", \"078\"\n",
    "}\n",
    "\n",
    "# --------------------------------------------------\n",
    "# REGEX (LOOSE MATCHING)\n",
    "# --------------------------------------------------\n",
    "\n",
    "PHONE_CANDIDATE_REGEX = re.compile(\n",
    "    r\"\"\"\n",
    "    (?:\n",
    "        \\+94|0\n",
    "    )\n",
    "    [\\s\\-]*\n",
    "    \\d{2}\n",
    "    (?:[\\s\\-]*\\d{3,4}){2}\n",
    "    \"\"\",\n",
    "    re.VERBOSE\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# NORMALIZATION\n",
    "# --------------------------------------------------\n",
    "\n",
    "def normalize_sri_lankan_number(raw: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize Sri Lankan phone numbers to 0XXXXXXXXX\n",
    "    \"\"\"\n",
    "    digits = re.sub(r\"\\D\", \"\", raw)\n",
    "\n",
    "    # +94XXXXXXXXX → 0XXXXXXXXX\n",
    "    if digits.startswith(\"94\") and len(digits) == 11:\n",
    "        return \"0\" + digits[2:]\n",
    "\n",
    "    # Already local format\n",
    "    if digits.startswith(\"0\") and len(digits) == 10:\n",
    "        return digits\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "# VALIDATION\n",
    "# --------------------------------------------------\n",
    "\n",
    "def validate_landline_number(number: str) -> bool:\n",
    "    return (\n",
    "        len(number) == 10\n",
    "        and number[:3] in VALID_LANDLINE_AREA_CODES\n",
    "    )\n",
    "\n",
    "def validate_mobile_number(number: str) -> bool:\n",
    "    return (\n",
    "        len(number) == 10\n",
    "        and number[:3] in VALID_MOBILE_OPERATOR_CODES\n",
    "    )\n",
    "\n",
    "# --------------------------------------------------\n",
    "# EXTRACTION (MAIN FUNCTION)\n",
    "# --------------------------------------------------\n",
    "\n",
    "def extract_phone_numbers(text: str):\n",
    "    \"\"\"\n",
    "    Extracts Sri Lankan mobile & landline numbers from text\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for match in PHONE_CANDIDATE_REGEX.finditer(text):\n",
    "        raw_value = match.group()\n",
    "        normalized = normalize_sri_lankan_number(raw_value)\n",
    "\n",
    "        if not normalized:\n",
    "            continue\n",
    "\n",
    "        if validate_mobile_number(normalized):\n",
    "            phone_type = \"MOBILE\"\n",
    "        elif validate_landline_number(normalized):\n",
    "            phone_type = \"LANDLINE\"\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        results.append({\n",
    "            \"entity\": \"PHONE_NUMBER\",\n",
    "            \"value\": raw_value,\n",
    "            \"start\": match.start(),\n",
    "            \"end\": match.end(),\n",
    "            \"source\": \"regex\",\n",
    "            \"confidence\": 1.0\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f27e8e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'PHONE_NUMBER', 'value': '011 2345678', 'start': 5, 'end': 16, 'source': 'regex', 'confidence': 1.0}, {'entity': 'PHONE_NUMBER', 'value': '0771234567', 'start': 20, 'end': 30, 'source': 'regex', 'confidence': 1.0}, {'entity': 'PHONE_NUMBER', 'value': '0759876543', 'start': 34, 'end': 44, 'source': 'regex', 'confidence': 1.0}]\n"
     ]
    }
   ],
   "source": [
    "text = \"Call 011 2345678 or 0771234567 or 0759876543\"\n",
    "\n",
    "print(extract_phone_numbers(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d8ee43",
   "metadata": {},
   "source": [
    "IP ADDRESS DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38b516d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipaddress\n",
    "\n",
    "IPV4_REGEX = re.compile(r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\")\n",
    "\n",
    "IPV6_REGEX = re.compile(\n",
    "    r\"\\b(?:[0-9a-fA-F]{0,4}:){2,7}[0-9a-fA-F]{0,4}\\b\"\n",
    ")\n",
    "\n",
    "def is_valid_ipv4(ip: str) -> bool:\n",
    "    parts = ip.split(\".\")\n",
    "    if len(parts) != 4:\n",
    "        return False\n",
    "\n",
    "    for part in parts:\n",
    "        if not part.isdigit():\n",
    "            return False\n",
    "        if not 0 <= int(part) <= 255:\n",
    "            return False\n",
    "\n",
    "        # Prevent leading zeros like 001\n",
    "        if part != \"0\" and part.startswith(\"0\"):\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def is_valid_ipv6(ip: str) -> bool:\n",
    "    try:\n",
    "        ipaddress.IPv6Address(ip)\n",
    "        return True\n",
    "    except ipaddress.AddressValueError:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    \n",
    "def extract_ip_addresses(text: str):\n",
    "    results = []\n",
    "\n",
    "    # IPv4\n",
    "    for match in IPV4_REGEX.finditer(text):\n",
    "        token = match.group()\n",
    "        if is_valid_ipv4(token):\n",
    "            results.append({\n",
    "                \"entity\": \"IP_ADDRESS_V4\",\n",
    "                \"value\": token,\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end(),\n",
    "                \"source\": \"regex\",\n",
    "                \"confidence\": 1.0\n",
    "            })\n",
    "\n",
    "    # IPv6\n",
    "    for match in IPV6_REGEX.finditer(text):\n",
    "        token = match.group()\n",
    "        if is_valid_ipv6(token):\n",
    "            results.append({\n",
    "                \"entity\": \"IP_ADDRESS_V6\",\n",
    "                \"value\": token,\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end(),\n",
    "                \"source\": \"regex\",\n",
    "                \"confidence\": 1.0\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dbb355",
   "metadata": {},
   "source": [
    "MAC ADDRESS DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b09f63da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "MAC_COLON_HYPHEN_REGEX = re.compile(\n",
    "    r\"\\b(?:[0-9A-Fa-f]{2}[:-]){5}[0-9A-Fa-f]{2}\\b\"\n",
    ")\n",
    "\n",
    "MAC_DOT_REGEX = re.compile(\n",
    "    r\"\\b(?:[0-9A-Fa-f]{4}\\.){2}[0-9A-Fa-f]{4}\\b\"\n",
    ")\n",
    "\n",
    "def is_valid_mac(mac: str) -> bool:\n",
    "    # Normalize\n",
    "    mac_clean = mac.replace(\":\", \"\").replace(\"-\", \"\").replace(\".\", \"\")\n",
    "    \n",
    "    if len(mac_clean) != 12:\n",
    "        return False\n",
    "    \n",
    "    if not all(c in \"0123456789abcdefABCDEF\" for c in mac_clean):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def get_mac_admin_type(mac: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - 'UAA' (Universally Administered Address)\n",
    "    - 'LAA' (Locally Administered Address)\n",
    "    \"\"\"\n",
    "    first_octet = mac.replace(\":\", \"\").replace(\"-\", \"\").replace(\".\", \"\")[:2]\n",
    "    first_octet_int = int(first_octet, 16)\n",
    "\n",
    "    # Check U/L bit (bit 1)\n",
    "    if first_octet_int & 0b00000010:\n",
    "        return \"LAA\"\n",
    "    else:\n",
    "        return \"UAA\"\n",
    "\n",
    "def extract_mac_addresses(text: str):\n",
    "    results = []\n",
    "\n",
    "    for match in MAC_COLON_HYPHEN_REGEX.finditer(text):\n",
    "        token = match.group()\n",
    "        if is_valid_mac(token):\n",
    "            results.append({\n",
    "                \"entity\": \"MAC_ADDRESS\",\n",
    "                \"value\": token,\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end(),\n",
    "                \"source\": \"regex\",\n",
    "                \"confidence\": 1.0\n",
    "            })\n",
    "\n",
    "    for match in MAC_DOT_REGEX.finditer(text):\n",
    "        token = match.group()\n",
    "        if is_valid_mac(token):\n",
    "            results.append({\n",
    "                \"entity\": \"MAC_ADDRESS\",\n",
    "                \"value\": token,\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end(),\n",
    "                \"source\": \"regex\",\n",
    "                \"confidence\": 1.0\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22622bc",
   "metadata": {},
   "source": [
    "DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35348087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "DATE_CANDIDATE_REGEX = re.compile(\n",
    "    r\"\"\"\n",
    "    (\n",
    "        # Numeric formats with separators (/ - . space)\n",
    "        \\b\\d{1,4}[\\s\\-\\/\\.]\\d{1,2}[\\s\\-\\/\\.]\\d{1,4}\\b\n",
    "        |\n",
    "        # Text month formats (5 January 2025)\n",
    "        \\b\\d{1,2}\\s+\n",
    "        (Jan|January|Feb|February|Mar|March|Apr|April|May|\n",
    "         Jun|June|Jul|July|Aug|August|Sep|September|Oct|October|\n",
    "         Nov|November|Dec|December)\n",
    "        \\s+\\d{2,4}\\b\n",
    "        |\n",
    "        # Text month formats (January 5, 2025)\n",
    "        \\b(Jan|January|Feb|February|Mar|March|Apr|April|May|\n",
    "           Jun|June|Jul|July|Aug|August|Sep|September|Oct|October|\n",
    "           Nov|November|Dec|December)\n",
    "        \\s+\\d{1,2},?\\s+\\d{2,4}\\b\n",
    "    )\n",
    "    \"\"\",\n",
    "    re.IGNORECASE | re.VERBOSE\n",
    ")\n",
    "\n",
    "\n",
    "DATE_FORMATS = [\n",
    "    # Day first\n",
    "    \"%d/%m/%Y\", \"%d-%m-%Y\", \"%d.%m.%Y\", \"%d %m %Y\",\n",
    "    \"%d/%m/%y\", \"%d-%m-%y\", \"%d %m %y\",\n",
    "\n",
    "    # Year first\n",
    "    \"%Y-%m-%d\", \"%Y/%m/%d\", \"%Y %m %d\",\n",
    "\n",
    "    # Textual\n",
    "    \"%d %B %Y\", \"%d %b %Y\",\n",
    "    \"%B %d %Y\", \"%B %d, %Y\",\n",
    "    \"%b %d %Y\", \"%b %d, %Y\",\n",
    "]\n",
    "\n",
    "def normalize_date(raw: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize dates to ISO format: YYYY-MM-DD\n",
    "    \"\"\"\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", raw.strip())\n",
    "\n",
    "    for fmt in DATE_FORMATS:\n",
    "        try:\n",
    "            dt = datetime.strptime(cleaned, fmt)\n",
    "            return dt.strftime(\"%Y-%m-%d\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def validate_date(normalized: str) -> bool:\n",
    "    try:\n",
    "        datetime.strptime(normalized, \"%Y-%m-%d\")\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def extract_dates(text: str):\n",
    "    \"\"\"\n",
    "    Extracts dates from text in multiple formats\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for match in DATE_CANDIDATE_REGEX.finditer(text):\n",
    "        raw_value = match.group()\n",
    "        normalized = normalize_date(raw_value)\n",
    "\n",
    "        if not normalized:\n",
    "            continue\n",
    "\n",
    "        if not validate_date(normalized):\n",
    "            continue\n",
    "\n",
    "        results.append({\n",
    "            \"entity\": \"DATE\",\n",
    "            \"value\": normalized,\n",
    "            \"raw\": raw_value,\n",
    "            \"start\": match.start(),\n",
    "            \"end\": match.end(),\n",
    "            \"source\": \"regex\",\n",
    "            \"confidence\": 1.0\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d1d8319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'DATE', 'value': '1994-08-12', 'raw': '12 08 1994', 'start': 6, 'end': 16, 'source': 'regex', 'confidence': 1.0}, {'entity': 'DATE', 'value': '2025-01-05', 'raw': '5 January 2025', 'start': 29, 'end': 43, 'source': 'regex', 'confidence': 1.0}, {'entity': 'DATE', 'value': '1994-08-12', 'raw': '1994-08-12', 'start': 49, 'end': 59, 'source': 'regex', 'confidence': 1.0}, {'entity': 'DATE', 'value': '1994-12-08', 'raw': '08/12/94', 'start': 67, 'end': 75, 'source': 'regex', 'confidence': 1.0}]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "DOB: 12 08 1994\n",
    "Visit date: 5 January 2025\n",
    "ISO: 1994-08-12\n",
    "Short: 08/12/94\n",
    "\"\"\"\n",
    "\n",
    "print(extract_dates(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67140dc1",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4356780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spans_overlap(a, b):\n",
    "    return not (a[\"end\"] <= b[\"start\"] or b[\"end\"] <= a[\"start\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6799cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITY_PRIORITY = {\n",
    "    \"NIC\": 100,\n",
    "    \"CREDIT_CARD\": 100,\n",
    "    \"BANK_ACCOUNT\": 90,\n",
    "    \"PHONE_LANDLINE\": 80,\n",
    "    \"IP_ADDRESS\": 70,\n",
    "    \"MAC_ADDRESS\": 70,\n",
    "\n",
    "    # NER entities (lower priority)\n",
    "    \"PER\": 30,\n",
    "    \"ORG\": 30,\n",
    "    \"LOC\": 30,\n",
    "    \"MISC\": 20\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cc0edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_entities(entities):\n",
    "    \"\"\"\n",
    "    Resolves overlapping entities using priority + confidence.\n",
    "    Deterministic rules win over NER.\n",
    "    \"\"\"\n",
    "\n",
    "    merged = []\n",
    "\n",
    "    for e in sorted(entities, key=lambda x: x[\"start\"]):\n",
    "        keep = True\n",
    "\n",
    "        for m in merged:\n",
    "            if spans_overlap(e, m):\n",
    "\n",
    "                p_e = ENTITY_PRIORITY.get(e[\"entity\"], 10)\n",
    "                p_m = ENTITY_PRIORITY.get(m[\"entity\"], 10)\n",
    "\n",
    "                # Higher priority wins\n",
    "                if p_e > p_m:\n",
    "                    merged.remove(m)\n",
    "                    continue\n",
    "\n",
    "                # Same priority → higher confidence wins\n",
    "                if p_e == p_m and e[\"confidence\"] > m[\"confidence\"]:\n",
    "                    merged.remove(m)\n",
    "                    continue\n",
    "\n",
    "                # Otherwise discard current entity\n",
    "                keep = False\n",
    "                break\n",
    "\n",
    "        if keep:\n",
    "            merged.append(e)\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f88ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pii(text: str):\n",
    "    results = []\n",
    "\n",
    "    # Regex-based entities\n",
    "    results.extend(extract_nic(text))\n",
    "    results.extend(extract_phone_numbers(text))\n",
    "    results.extend(extract_ip_addresses(text))\n",
    "    results.extend(extract_mac_addresses(text))\n",
    "    results.extend(extract_dates(text))\n",
    "\n",
    "    # NER entities\n",
    "    results.extend(run_ner(text))\n",
    "    results.extend(run_ner2(text))\n",
    "\n",
    "    # Optional: sort by start position\n",
    "    results = sorted(results, key=lambda x: x[\"start\"])\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ca9616e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'ORG', 'value': 'Amanda Arangalla', 'start': 2, 'end': 18, 'source': 'ner', 'confidence': 0.888}\n",
      "{'entity': 'NIC', 'value': '200255701652', 'start': 20, 'end': 32, 'source': 'regex', 'confidence': 1.0}\n",
      "{'entity': 'PASSWORD', 'value': '200255701652,', 'start': 20, 'end': 33, 'source': 'ner2', 'confidence': 0.85}\n",
      "{'entity': 'PHONE_NUMBER', 'value': '0312220293', 'start': 34, 'end': 44, 'source': 'regex', 'confidence': 1.0}\n",
      "{'entity': 'PASSWORD', 'value': '0312220293,', 'start': 34, 'end': 45, 'source': 'ner2', 'confidence': 0.85}\n",
      "{'entity': 'PHONE_NUMBER', 'value': '0779381115', 'start': 46, 'end': 56, 'source': 'regex', 'confidence': 1.0}\n",
      "{'entity': 'PASSWORD', 'value': '0779381115, Negombo, Mega Trend Lanka (Pvt)', 'start': 46, 'end': 89, 'source': 'ner2', 'confidence': 0.85}\n",
      "{'entity': 'ORG', 'value': 'Negombo, Mega Trend Lanka (Pvt) Ltd', 'start': 58, 'end': 93, 'source': 'ner', 'confidence': 0.922}\n",
      "{'entity': 'EMAIL', 'value': 'arangallaamanda@gmail.com}', 'start': 95, 'end': 121, 'source': 'ner2', 'confidence': 0.85}\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "{Amanda Arangalla, 200255701652, 0312220293, 0779381115, Negombo, Mega Trend Lanka (Pvt) Ltd, arangallaamanda@gmail.com}\n",
    "\"\"\"\n",
    "\n",
    "entities = extract_pii(text)\n",
    "\n",
    "for e in entities:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9239f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_policy_map(policy_file_path):\n",
    "    \n",
    "    with open(policy_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    priority = {\"High\": 3, \"Medium\": 2, \"Low\": 1}\n",
    "    policy_map = {}\n",
    "\n",
    "    for policy in data[\"policies\"]:\n",
    "        entity = policy[\"entity\"].strip().upper()\n",
    "        level = policy[\"sensitivity_level\"]\n",
    "\n",
    "        # keep the highest sensitivity if duplicates exist\n",
    "        if entity not in policy_map or priority[level] > priority[policy_map[entity]]:\n",
    "            policy_map[entity] = level\n",
    "\n",
    "    return policy_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7397d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sensitivity_levels(detected_entities, policy_map):\n",
    "    \"\"\"\n",
    "    Adds sensitivity_level to each detected entity\n",
    "    \"\"\"\n",
    "    enriched = []\n",
    "\n",
    "    for ent in detected_entities:\n",
    "        entity_type = ent[\"entity\"].strip().upper()\n",
    "\n",
    "        ent[\"sensitivity_level\"] = policy_map.get(entity_type, \"Unknown\")\n",
    "        enriched.append(ent)\n",
    "\n",
    "    return enriched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e3be14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Patient Dinithi Rajapaksha, born on 12/08/1994, visited the Colombo National Hospital on 5 January 2025.\n",
    "She can be contacted via dinithi.rajapaksha@gmail.com or +94 77 456 7890.\n",
    "Her NIC number is 911042754V and she currently resides at No. 45, Temple Road, Kandy, Sri Lanka.\n",
    "The patient has a medical history of diabetes mellitus and hypertension.\n",
    "Her insurance policy ID is INS-SL-889233, issued on 2023-06-15.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "308dec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"im nanduni. from kandy . 0771373112. nadu@gmail.com\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65bd6dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sensitivity(text):\n",
    "    entities = extract_pii(text)\n",
    "    policy_map = load_policy_map(\"../policy_engine.json\")\n",
    "    enriched_entities = add_sensitivity_levels(entities, policy_map)\n",
    "    return enriched_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d26e3f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'PERSON',\n",
       "  'value': 'im nanduni.',\n",
       "  'start': 0,\n",
       "  'end': 11,\n",
       "  'source': 'ner2',\n",
       "  'confidence': 0.85,\n",
       "  'sensitivity_level': 'High'},\n",
       " {'entity': 'EMAIL',\n",
       "  'value': 'kandy',\n",
       "  'start': 17,\n",
       "  'end': 22,\n",
       "  'source': 'ner2',\n",
       "  'confidence': 0.85,\n",
       "  'sensitivity_level': 'High'},\n",
       " {'entity': 'PHONE_NUMBER',\n",
       "  'value': '0771373112',\n",
       "  'start': 25,\n",
       "  'end': 35,\n",
       "  'source': 'regex',\n",
       "  'confidence': 1.0,\n",
       "  'sensitivity_level': 'Medium'},\n",
       " {'entity': 'EMAIL',\n",
       "  'value': '0771373112.',\n",
       "  'start': 25,\n",
       "  'end': 36,\n",
       "  'source': 'ner2',\n",
       "  'confidence': 0.85,\n",
       "  'sensitivity_level': 'High'},\n",
       " {'entity': 'EMAIL',\n",
       "  'value': 'nadu@gmail.com',\n",
       "  'start': 37,\n",
       "  'end': 51,\n",
       "  'source': 'ner2',\n",
       "  'confidence': 0.85,\n",
       "  'sensitivity_level': 'High'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sensitivity(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "413aa2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'PERSON',\n",
       "  'value': 'im nanduni.',\n",
       "  'start': 0,\n",
       "  'end': 11,\n",
       "  'source': 'ner2',\n",
       "  'confidence': 0.85,\n",
       "  'sensitivity_level': 'High'},\n",
       " {'entity': 'EMAIL',\n",
       "  'value': 'kandy',\n",
       "  'start': 17,\n",
       "  'end': 22,\n",
       "  'source': 'ner2',\n",
       "  'confidence': 0.85,\n",
       "  'sensitivity_level': 'High'},\n",
       " {'entity': 'PHONE_NUMBER',\n",
       "  'value': '0771373112',\n",
       "  'start': 25,\n",
       "  'end': 35,\n",
       "  'source': 'regex',\n",
       "  'confidence': 1.0,\n",
       "  'sensitivity_level': 'Medium'},\n",
       " {'entity': 'EMAIL',\n",
       "  'value': '0771373112.',\n",
       "  'start': 25,\n",
       "  'end': 36,\n",
       "  'source': 'ner2',\n",
       "  'confidence': 0.85,\n",
       "  'sensitivity_level': 'High'},\n",
       " {'entity': 'EMAIL',\n",
       "  'value': 'nadu@gmail.com',\n",
       "  'start': 37,\n",
       "  'end': 51,\n",
       "  'source': 'ner2',\n",
       "  'confidence': 0.85,\n",
       "  'sensitivity_level': 'High'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "policy_map = load_policy_map(\"../policy_engine.json\")\n",
    "entities = extract_pii(text)  # this returns the list of dicts as you showed\n",
    "\n",
    "# Step 3: Enrich entities with sensitivity_level\n",
    "enriched_entities = add_sensitivity_levels(entities, policy_map)\n",
    "enriched_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77996254",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
